{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n#Import the data and drop unnamed:0 column\ntrain = pd.read_csv(r'D:\\Subodh\\Projects\\Credit Risk\\Calculated PD\\Data\\cs-training_1.csv')\ntest = pd.read_csv(r'D:\\Subodh\\Projects\\Credit Risk\\Calculated PD\\Data\\cs-test_1.csv')\ntrain = train.drop(['Unnamed: 0'], axis=1)\ntest = test.drop(['Unnamed: 0'], axis=1)\ntrain.head()\ntest.head()\ntrain.shape\n\ntest.shape\n#Find duplicates\ntrain.duplicated().value_counts()\n#Drop duplicates\ntrain_redup = train.drop_duplicates()\n#Checking if duplicates still persist\ntrain_redup.duplicated().sum()\n#Check for null \ntrain_redup.isnull().sum()\n# % of null values\nround(train_redup.isnull().sum()/train_redup.shape[0]*100,2)\ndef findMiss(df):\n    return round(df.isnull().sum()/df.shape[0]*100,2)\n\nfindMiss(train_redup)\n#Filtering data where null values are present for no. of dependents\ntrain_redup[train_redup.NumberOfDependents.isnull()]\n\ntrain_redup[train_redup.NumberOfDependents.isnull()].describe()\n#We observe that monthly income is not present wherein no. of dependents data is null\ntrain_redup[train_redup.MonthlyIncome.isnull()].describe()\n#We observe that maximum no of depedents are 9 where monthyly income is null\ntrain_redup['NumberOfDependents'].agg(['mode'])\n#this means most of the rows have zero dependendts\ntrain_redup.groupby(['NumberOfDependents']).size()\n#Create a subset of dataset and fill null as 0 \nfam_miss = train_redup[train_redup.NumberOfDependents.isnull()]\nfam_nmiss = train_redup[train_redup.NumberOfDependents.notnull()]\n \nfam_miss.shape\n\nfam_nmiss.shape\nfam_miss['NumberOfDependents'] = fam_miss['NumberOfDependents'].fillna(0)\nfam_miss['MonthlyIncome'] =  fam_miss['MonthlyIncome'].fillna(0)\n#recheck for missing values\nfindMiss(fam_miss)\nfindMiss(fam_nmiss)\n#evaluate\n\nfam_nmiss['MonthlyIncome'].agg(['mean','median', 'min'])\n\n#We observe that mean and median are not close to each other. Also mean value is 0. Hence, not a good idea to fill null with min values. \nfam_nmiss['MonthlyIncome'].agg(['max'])\n\n#We can conclude that there are definitely some outliers in the data. Hence, we would go forward with the median values. \nfam_nmiss['MonthlyIncome'] = fam_nmiss['MonthlyIncome'].fillna(fam_nmiss['MonthlyIncome'].median())\n#Check for missing values\nfindMiss(fam_nmiss)\n#Concat both the dataset\nfilled_train = pd.concat([fam_nmiss, fam_miss], axis=0)\n\nfilled_train.shape\n#Check for missing values in concat datset\nfindMiss(filled_train)\nfilled_train.head()\n#Finding the proportion of delinq and non-delinq customers\nfilled_train.groupby(['SeriousDlqin2yrs']).size()/filled_train.shape[0]\n#93% are non-delinq and approx 6.7% are delinq\n\n#Biased data\n\n#Hence analyze the data on each variable and figure out which rows can be removed from the data for clean analysis \nfilled_train.RevolvingUtilizationOfUnsecuredLines.describe()\nfilled_train['RevolvingUtilizationOfUnsecuredLines'].quantile([.5])\nfilled_train['RevolvingUtilizationOfUnsecuredLines'].quantile([.9])\nfilled_train['RevolvingUtilizationOfUnsecuredLines'].quantile([.95])\nfilled_train['RevolvingUtilizationOfUnsecuredLines'].quantile([.99])\n# Upto 99th percentile data lies between 0 and 1. Clearly there are some outliers in the data \n#Let's divide the data and analyze\n\nfilled_train[filled_train['RevolvingUtilizationOfUnsecuredLines'] < 1].describe()\n\n#146053 records are less than 1\nfilled_train[filled_train['RevolvingUtilizationOfUnsecuredLines'] > 1].describe()\n# 3321 records have values greater than 1\n# avg default rate is 0.372478\n\nfilled_train[filled_train['RevolvingUtilizationOfUnsecuredLines'] > 1].describe()\n\nfilled_train[filled_train['RevolvingUtilizationOfUnsecuredLines'] > 10].describe().groupby(['SeriousDlqin2yrs']).size()\n# 241 records have values greater than 10\n# avg default rate is 0.070539\nfilled_train[filled_train['RevolvingUtilizationOfUnsecuredLines'] > 10].groupby(['SeriousDlqin2yrs']).size()\n#out of 241, only 17 are defaulters\nfilled_train[filled_train['RevolvingUtilizationOfUnsecuredLines'] > 1].groupby(['SeriousDlqin2yrs']).size()\n\n#We cannot simply remove the data where the value is greater than 1. It will make the data more imbalanced\n#we will remove the values greater than 10 considering them as outliers\n#Ideally we should not consider values more than 1 but after analyzing the data we might miss some important info\n\n#Checking for impact on other variables\nfilled_train[filled_train['RevolvingUtilizationOfUnsecuredLines'] > 10].describe()\n\nuntil_dropped = filled_train.drop(filled_train[filled_train['RevolvingUtilizationOfUnsecuredLines'] > 10].index)\n\nuntil_dropped.shape\n\n#records came down from 149391\nuntil_dropped.head()\n#Checking for outliers in age with boxplot\n\nsns.boxplot(data=until_dropped, x='age')\n\n#We can see the outlier where age is zero which is not possible \n\n#We can go ahead and remove this. We can revert while training the model\nuntil_dropped.groupby(['NumberOfTime30-59DaysPastDueNotWorse']).size()\n\n#No values between 13 and 96\n#We can observe the outliers where customers have delayed the payments 96,98 times but not for 14 or 15 times\n#Looks like an error in data but let's check for no. of defaulters\n\n#Let's analze all the delayed payments\n\nuntil_dropped.groupby(['NumberOfTime60-89DaysPastDueNotWorse']).size()\n\nuntil_dropped.groupby(['NumberOfTimes90DaysLate']).size()\n\n\n#After 17 times the nearest number is 96 in all the three dataset\n#Checking for impact \nuntil_dropped[until_dropped['NumberOfTimes90DaysLate'] >=96 ]['SeriousDlqin2yrs'].describe()\n#About 60% are delinq\nuntil_dropped[until_dropped['NumberOfTimes90DaysLate'] >=96 ].groupby(['SeriousDlqin2yrs']).size()\n#There a good chunk of values greater than 1 hence would not be good idea to delete this. let's check for other ways to treat this data\n#We will try to winsorizing. Treating outliers to come closer to the other values\n#Pending variable = debt ratio\n\nuntil_dropped['DebtRatio'].describe()\n\nsns.kdeplot(until_dropped['DebtRatio'])\n\n#most of the points between 0 and 50000\n#finding the proportion of outliers\n\nuntil_dropped['DebtRatio'].quantile([.9])\n\nuntil_dropped['DebtRatio'].quantile([.5])\n\nuntil_dropped['DebtRatio'].quantile([.75])\n\nuntil_dropped['DebtRatio'].quantile([.95])\n\nuntil_dropped['DebtRatio'].quantile([.975])\n\nuntil_dropped[until_dropped['DebtRatio']>3000].describe()\n#There are 5226 records have values more than 3000\nuntil_dropped[until_dropped['DebtRatio']>3500].describe()\n\n#Checking for delinq accounts\n\n#3492 is taken from 97.5 percentile\n\nuntil_dropped[until_dropped['DebtRatio']>3492].groupby(['SeriousDlqin2yrs']).size()\n\nuntil_dropped[until_dropped['DebtRatio']>3492].describe()\n\nuntil_dropped[until_dropped['DebtRatio']>3492][['SeriousDlqin2yrs', 'MonthlyIncome']].describe()\n\n\n#checking if for some of the records if monthly income and target variable are same(seriousdelin2yers)\n\nuntil_dropped[(until_dropped['DebtRatio'] >3492 ) & (until_dropped['SeriousDlqin2yrs'] == until_dropped['MonthlyIncome'])].describe()\n\n\n#Checking for delinq accounts\n\ntemp = until_dropped[(until_dropped['DebtRatio'] >3492 ) & (until_dropped['SeriousDlqin2yrs'] == until_dropped['MonthlyIncome'])]\n\ntemp.head()\ntemp.groupby(['SeriousDlqin2yrs']).size()\n\n#We can observe only 2 counts greater than 1 i.e delinq accounts. We can go ahead and remove the 382 values\ndRatio = until_dropped.drop(until_dropped[(until_dropped['DebtRatio']>3492) & (until_dropped['SeriousDlqin2yrs']==until_dropped['MonthlyIncome'])].index)\n\ndRatio.shape\ndRatio.head()\n\n#We have evaluated all the variables. We can ignore NumberRealEstateLoansOrLines\nuntil_dropped.shape\n#Checking if we have dropped the variables\ndRatio.shape\ndRatio.groupby('SeriousDlqin2yrs').size()/dRatio.shape[0]\n#We still have data imbalance hence we will go with xgboost\npip install Xgboost\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix, classification_report\nmodel = XGBClassifier(tree_method = 'exact')\nx = dRatio.drop(['SeriousDlqin2yrs'], axis=1) #dependent variable\n\nx.head()\ny = dRatio['SeriousDlqin2yrs'] #independent variable\n\nmodel.fit(x,y.values.ravel())\n\ny_pred = model.predict(x)\naccuracy_score(y,y_pred)\n\n#we are getting the low accuracy as data is not perfectly balanced\nconfusion_matrix(y,y_pred)\n#true negative = 137966. Not defaulters\n\ncm = confusion_matrix(y,y_pred)\n\nsns.heatmap(cm,annot=True,fmt='d',cmap='Oranges',linewidths=0.5,linecolor='Black')\nplt.xticks(np.arange(2)+.5,['No def','def'])\nplt.yticks(np.arange(2)+.5,['No def','def'])\nplt.xlabel(\"predicted\")\nplt.ylabel(\"actuals\")\n\n# 7144 are false negatives \nprint(classification_report(y,y_pred))\n#For the \"No default\" class (SeriousDlqin2yrs = 0), the precision is high, approximately 95%. This indicates that when the model predicts \"No default,\" it is correct about 95% of the time.\n#For the \"Default\" class (SeriousDlqin2yrs = 1), the precision is lower, about 78%. This means that when the model predicts \"Default,\" it is correct about 78% of the time.\n#For the \"No default\" class (SeriousDlqin2yrs = 0), the recall is high, approximately 99%. This indicates that the model correctly identifies about 99% of the actual \"No default\" cases.\n#For the \"Default\" class (SeriousDlqin2yrs = 1), the recall is lower, about 28%. This means that the model identifies only about 28% of the actual \"Default\" cases.\n#The F1-score is a balance between precision and recall. For the \"No default\" class, it's approximately 97%, indicating good overall performance.\n#For the \"Default\" class, it's much lower, about 42%, indicating that achieving a balance between precision and recall for this class is challenging.\n#The overall accuracy of the model is approximately 95%, which looks impressive at first glance. \n#However, it's important to consider the class imbalance in the dataset, where the majority of cases are \"No default.\"\n#The macro average of precision, recall, and F1-score is calculated by taking the average of these metrics for both classes, giving equal weight to each class. It is approximately 0.86.\n#The weighted average considers class imbalance and is approximately 0.94, indicating the overall weighted performance across both classes.\n#In summary, the model performs well in correctly identifying non-default cases (SeriousDlqin2yrs = 0) but struggles with identifying default cases (SeriousDlqin2yrs = 1), likely due to the class imbalance. \n#Further efforts to balance the dataset or explore different modeling techniques may improve the model's performance for default prediction.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]}]}